{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a real classifier\n",
    "\n",
    "This code is an example of the use of VADER classifier from NLTK. It is a Naive-Bayes classifier that is trainded with a lexicon and dataset of movie reviews.\n",
    "\n",
    "Look in the example how the library SKLearn is used to evaulate the classifier.\n",
    "\n",
    "At the end you have an example on how to use the classifier en custom examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/isaacgonzalez/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/isaacgonzalez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/isaacgonzalez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/isaacgonzalez/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Classifier Evaluation:\n",
      "Accuracy: 74.50%\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     16.3 : 1.0\n",
      "                  seagal = True              neg : pos    =     12.6 : 1.0\n",
      "                captures = True              pos : neg    =     11.7 : 1.0\n",
      "                    slip = True              pos : neg    =     11.3 : 1.0\n",
      "               insulting = True              neg : pos    =     11.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =     10.7 : 1.0\n",
      "                 tribute = True              pos : neg    =     10.6 : 1.0\n",
      "              astounding = True              pos : neg    =     10.0 : 1.0\n",
      "                  avoids = True              pos : neg    =     10.0 : 1.0\n",
      "                  darker = True              pos : neg    =     10.0 : 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.93      0.49      0.65       188\n",
      "         pos       0.68      0.97      0.80       212\n",
      "\n",
      "    accuracy                           0.74       400\n",
      "   macro avg       0.81      0.73      0.72       400\n",
      "weighted avg       0.80      0.74      0.73       400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 93  95]\n",
      " [  7 205]]\n",
      "\n",
      "VADER Sentiment Analysis:\n",
      "Sentence: I absolutely loved this movie! The acting was fantastic.\n",
      "Sentiment: positive (Score: 0.8436)\n",
      "\n",
      "Sentence: This was the worst film I have ever seen.\n",
      "Sentiment: negative (Score: -0.6249)\n",
      "\n",
      "Sentence: The plot was predictable, but the cinematography was beautiful.\n",
      "Sentiment: positive (Score: 0.7469)\n",
      "\n",
      "Sentence: I wouldn't recommend it. It was boring and too long.\n",
      "Sentiment: negative (Score: -0.5283)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "\n",
    "# Download required NLTK datasets\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Preprocess the data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_features(words):\n",
    "    return {word: True for word in words if word.lower() not in stop_words}\n",
    "\n",
    "# Prepare the dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)  # Shuffle the dataset for better randomness\n",
    "\n",
    "# Feature extraction\n",
    "feature_sets = [(extract_features(words), category) for (words, category) in documents]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(feature_sets) * 0.8)\n",
    "train_set, test_set = feature_sets[:train_size], feature_sets[train_size:]\n",
    "\n",
    "# Train a Naive Bayes Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"\\nNaive Bayes Classifier Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy(classifier, test_set) * 100:.2f}%\")\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "# Prepare predictions and true labels for sklearn metrics\n",
    "y_true = [label for (_, label) in test_set]\n",
    "y_pred = [classifier.classify(features) for (features, _) in test_set]\n",
    "\n",
    "# Evaluate using sklearn metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# VADER Sentiment Analysis on custom examples\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "example_sentences = [\n",
    "    \"I absolutely loved this movie! The acting was fantastic.\",\n",
    "    \"This was the worst film I have ever seen.\",\n",
    "    \"The plot was predictable, but the cinematography was beautiful.\",\n",
    "    \"I wouldn't recommend it. It was boring and too long.\"\n",
    "]\n",
    "\n",
    "print(\"\\nVADER Sentiment Analysis:\")\n",
    "for sentence in example_sentences:\n",
    "    score = sia.polarity_scores(sentence)\n",
    "    sentiment = \"positive\" if score['compound'] > 0 else \"negative\"\n",
    "    print(f\"Sentence: {sentence}\\nSentiment: {sentiment} (Score: {score['compound']})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "Create your own gold standard and measure Precission, Recall, and F1 manually and with SKLearn to check if the result is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually calculated precision: 0.6666666666666666\n",
      "Manually calculated recall: 0.8\n",
      "Manually calculated F1: 0.7272727272727272\n",
      "SKLearn calculated precision, recall, and F1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       0.67      0.80      0.73         5\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.71      0.70      0.70        10\n",
      "weighted avg       0.71      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Manually create your own gold standard\n",
    "true_labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "\n",
    "# Predicted labels from the classifier\n",
    "predicted_labels = [0, 1, 0, 0, 1, 1, 0, 1, 1, 1]\n",
    "\n",
    "# Calculate precision, recall, and F1 manually\n",
    "true_positive = sum([1 for true, pred in zip(true_labels, predicted_labels) if true == 1 and pred == 1])\n",
    "false_positive = sum([1 for true, pred in zip(true_labels, predicted_labels) if true == 0 and pred == 1])\n",
    "false_negative = sum([1 for true, pred in zip(true_labels, predicted_labels) if true == 1 and pred == 0])\n",
    "\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Calculate precision, recall, and F1 using SKLearn\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Manually calculated precision:\", precision)\n",
    "print(\"Manually calculated recall:\", recall)\n",
    "print(\"Manually calculated F1:\", f1)\n",
    "\n",
    "print(\"SKLearn calculated precision, recall, and F1:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
